---
layout:     post
title:      "Desiging Data-Intensive Application 读后感"
subtitle:   "The best book about data"
date:       2020-03-21 10:38:00
author:     "aguozhang"
header-img: "img/post-bg-2015.jpg"
tags:
    - ddia 
    - data 
    - good book
---

## 前言
关于数据的一本书，内容很全面，深入浅出，浅显易懂的典范，重要的是要多读几遍，慢慢的品。

读书不能急，要慢慢的研磨，慢慢的想。

慢工出细活

吃太快只是浪费时间

## 摘录
* there are enduring principles that remain true
* thinking about how and why
* Reliability
* Scalability
* Desribing load and performance
* Twitter home timelie algorithm
* Percentiles, tail latencies
* service level objectives, service level agreements
* Maitainability 
* Operability Simplicity Evolvability
* Operability: Making life easy for operations
* Simplicity: Managing complexity
* Evolvability: Making change easy

* Data Models and query languages
* how we think about the problem that we are solving
* ORM for mismatch
* self-contained document
* The json representation has better locality than the multitable schema
* Removing such duplication is the key idea behind normalization in database
* Are document databases repeating history?
* the network model
* the relational model
* schema flexibility in the document model
* schema on read and schema on write 
* a schema may hurt more than it helps and schemaless documents can be a much more natural data model
* data locality for queries
* convergence of document and relational databases
* delarative query language
* imperative language
* CSS on web as an example 
* Mapreduce querying is in the middle 
* graph like data models
* property graphs

* storage and retrieval
* when you give it some data, it should store the data, and when you ask it again later, it should give the data back to you
* log structured storage engine and page-oriented storage engines such as B-trees
* Hash indexes
* The hash table must fit in the memory and range queries are not efficient
* SSTables and LSM-Trees
* Sorted string table
* Maintaining a sorted structure on disk is possible, but maintaining it in memory is musch easier. Red black trees or AVL trees
* Making an LSM-tree out of sstables
* Bloon filters
* keeping a cascase of sstables that are merged in the background
* B-Trees
* Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-values lookups and range queries
* B-trees break the database down into fixed-size blocks or pages, traditionally 4KB in size and read or write one page at a time
* branching factor
* a four level tree of 4KB pages with a  branching factor of 500 can store up to 256TB
* WAL write ahead log
* B-trees attractive in databases that want to offer strong transactinal semantics
* B-trees are very ingrained in the architecutre of databases and provide consistently good performance for many workloads
* Other indexing structures
* multi-column indexes 
* the most common type of multi-column index is called a  concatenated index
* transaction processing or analytics
* acid
* data warehousing
* ETL 
* the divergence between OLTP databases and data warehouses
* stars and snowflakes: schemas for analytics
* dimension tables. As each row in the fact table represents an event, the dimensions represent the who, what, where, when, how and why of the event
* Snow flake schemas are more normalized than star schemas, but star schemas are often preferred because they are simpler for analysts to work with
* Column-Oriented storage
* Column compression
* Memory bandwidth and vectorized processing
* This technique is known as vectorized processing
* Aggregatin: data cubes and materialized views
* when the underlying data changes, a materialized views needs to be updated, because it is a denormalized copy of the data

* encoding and evolution
* evolvability
* rolling upgrade  or staged rollout
* backward compatibility
* forward compatibility
* encoding serialization marshalling
* decoding parsing deserialization unmarshalling
* Language-specific formats
* Json xml and binary variants
* thrift and protocol buffers
* fields tags and schema evolution
* datatypes and schema evolution
* Avro
* The writer's schema and the reader's schema
* The key idea with Avro is that the writer's schema and the reader's schema don't have to be the same -- they only need to be compatible
* Schema evolution rules
* But what is the writer's schema
* Large file with lots of records
* Database with individually written records
* Sending records over a network connection
* The difference is that Avro is friendlier to dynamically generated schemas
* Code generation and dynamically typed languages
* Modes of dataflow
* Dataflow through databases
* in that case you can think of storing something in the database as sending a message to your future self
* Dataflow through services: Rest and rpc
* Rest is not a protocol, but rather a design philosophy
* The problems with remote procedure calls(RPCs)
* The rpc model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process
* Rest doesn't try to hide the fact that it's a network protocol
* gRPC Finagle Rest.li
* Message-Passing Dataflow

* Distributed data
* scalability
* fault tolerance / high availability
* Latency
* Scaling to higher load
* shared-nothing architectures

* Replicaion
* Leaders adn followers
* Finally, leadered-based replication is not restricted to only databases
* Sychronous versus asynchronous replication
* semi-synchronous
* setting up new followers
* Follower failure: Catch-up recovery
* Leader failure: Failover
* Implementation of replication logs
* statement-based replication
* Write-ahead log(WAL) shipping
* Logical(row-based)log replication
* Trigger-based replication
* Problems with replication lag
* Reading your own writes
* Monotonic reads => always read from a same replica
* Consistent prefix reads => one solution is to make sure that any writes that are casually related to each other are written to the same partition
* Solutions for replication lag
* Multi-leader replicaion
* Clients with offline operation
* collaborative editing
* Handling write conflicts
* Multi-leader replication topologies
* Leaderless replication
* Read repair
* Anti-entropy process
* Quorums for reading and writing
* Limitations of quorum consistency
* Monitoring staleness
* Sloppy Quorums and hinted handoff
* Last write wins
* The happens-before relationship and concurrency
* Mering concurrency written values
* Version vectors
* We examined an algorithm that a database might use to determine whether one operation happened before another, or whether they happened concurrently

* Partitioning
* The main reason for wanting to partitin data is scalability
* Partition of Key-Value data
* skewed
* hot spot
* partitioning by key range
* partitioning by hash of key
* Consistent hashing
* Skewed workloads and relieving hot spots
* Partitioning and secondary indexes
* partitioning secondary indexes by document
* Partitioning secondary idexes by term
* Rebalancing partitions
* Strategies for rebalancing
* hash mod N 
* fixed number of partitions
* Operations: Automatic or Manaual rebalancing
* Request routing
* Parallel query execution
* Massively parallel processing MPP

* Transactions
* They were created with a purpose, namely to simplify the programming model for applications accessing a database
* safety guarantees
* read committed, snapshot isolation, serializability
* The meaning of acid
* BASE
* consistency is a property of the application
* Single-object and multi-object operations
* read modif write
* A key feature of a transaction is that it can be aborted and safely retried if an error occurred
* Weak isolation levels
* read committed
* Preventing lost updates
* remembers both the old committed value and the new value set by the transaction that currently holds the write lock
* snapshot isoloation and repeated read
* readers never block writers, and writers never block readers
* lost updates
* atomic write operations
* Explicit locking
* Automatically detecting lost updates
* Compare and set 
* Write skew and phantoms
* Serializability
* Two phase locking
* Serializable snapshot isolation

* The trouble with distributed systems
* Faults and partial failures
* I'm not even an ops guy
* Cloud computing and supercomputing
* Building a reliable system from unreliable components
* Unreliable networks
* Network faults in practice
* detecting faults
* timeouts and unbounded delays
* network congestion and queueing
* Synchronous versus asynchronous networks
* Latency and resource utilization
* Unreliable clocks
* durations and points in time
* Monotic versus time-of-day clocks
* clock synchronization and accuracy
* Relying on synchronized clocks
* timestamps for ordering events
* clocking readings have a confidence interval
* Process pauses
* a lease
* stop the world gc pauses
* Response time guarantees
* Limiting the impact of garbage collection
* Knowledge truth and lies
* the truth is defined by the majority
* the leader and the lock
* believe self is "the choose one"
* Fencing tokens
* zxid
* If zookeeper is used as lock service, the transaction ID zxid or the node version cversion can be used as fencing token
* Byzantine faults
* System model and reality 
* safty and liveness
* Mapping system models to the real world
* Partial failures
* Consistency and consensus
* the best way of building falult-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees
* consensus
* split brain
* linearizability
* give the illusion that there is only one replica data 
* linearizability  atomic consistency, strong consistency
* By recording the timings of all requests and responses and checking whether they can be arranged into a valid sequential order
* Serializability is an isolation property of transactions
* Linearizability is a recency guarantee on reads and writes of a register
* merge -> strict serializability or strong one-copy serializability
* Relying on linearizability
* Locking and leader selection
* Constraints and uniqueness guarantees
* Cross-channel timing dependencies
* Implementing linearizable systems
* Linearizability and quorums
* the CAP theorem
* either consistent or available when partitioned
* Linearizability and network delays
* Ordering guarantees
* Ordering and causality
* causes comes before effect
* the causal order is not a total order
* Linearizability is stronger than causual consistency
* capturing causal dependencies
* Sequence number ordering
* Lamport timestamps
* timestamp ordering is not sufficient
* Total order broadcast
* Reliable delivery 
* Totally ordered delivery
* implementing linearizable storage using total order broadcast
* implementing total order broadcast using linearizable storage
* Distributed transactions and consensus
* Leader election
* Atomic commit
* Atomic comit and two-phase commit (2PC)
* from single-node to distributed atomic commit
* a system of promises
* coordinator failure
* three phase commit 
* Distributed transactions in practice
* Exacting-once message processing
* XA transactions
* Holding locks while in doubt
* Fault-tolerant consensus
* Uniform agreement
* Integrity
* Validity
* Termination
* Membership and coordination services
* Allocating work to nodes

* Derived data
* Systems of record
* Derived data systems
* redundant
* denormalized

* Batch processing
* Batch processing with Unix tools
* Simple log analysis
* Chain of commands versus custom program
* sorting versus in-memory aggregation
* The unix philosophy
* Compose
* a uniform interface
* separation of logic and wiring
* Mapreduce and distributed filesystems
* Gfs namenode
* Mapreduce job execution
* mapper
* reducer
* mapreduce workflows
* reduce side joins and grouping
* Map side joins
* the output of batch workflows
* Building search indexes
* key-value stores as batch process output
* Philosophy of batch process outputs
* Designing for frequent faults
* Beyond mapreduce
* Materialization of intermediate state
* Dataflow engines
* Fault tolerance
* Discussion of materialization
* High level apis and languages

* Stream processing
* Event
* Messaging systems
* Direct messaging from producers to consumers
* Message brokers
* Message brokers compared to databases
* Partitioned logs
* Using logs for message storage
* Logs compared to traditional messaging
* Consumer offset
* Databases and streams
* Change data capture
* Implementing change data capture
* Event sourcing
* Deriving current state from the event log
* Commands and events
* States, streams, and immutability
* Deriving several views from the same event log
* CQRS
* Concurrency control
* Limitations of immutability
* performance
* excision
* Processing streams
* Uses of stream processing
* complex event processing
* stream analytics
* Maintaining materialized views
* search on strems
* Reasoning about time
* event time and processing time
* types of windows
* tumbling window
* Hopping window
* sliding window
* session window
* stream joins
* stream stream join
* stream table join
* table table join
* Fault tolerance
* microbatching and checkpointing
* atomic commit revisited

* The future of data systems
* data integration
* combining specialized tools by deriving data
* reasoning about dataflows
* derived data versus distributed transactions
* the limits of total ordering
* ordering event to capture causality
* Batch and stream processing
* maintaining derived state
* reprocessing data for application evolution
* the lambda architecture
* unifying batch and stream processing
* Unbundling databases
* composing data storage technologies
* creating an index
* the meta-database of everything
* Federated databases: unifying reads
* Unbundled databases: unifying writes
* Making unbundling work
* Designing applications around dataflow
* application code as a derivation function
* separation of application code and state
* dataflow: interplay between state change and application code
* stream processors and services
* observing derived state
* Materialized views and caching
* pushing state changes to clients
* read are events too
* Aiming for correctness
* reliable and correct
* the end-to-end argument for databases
* exactly-once execution of an operation
* duplicate suppression
* operation identifiers
* the end-to-end argument
* applying end-to-end thinking in data systems
* Enforcing constraints
* uniqueness constraints require consensus
* uniqueness in log-based messaging
* Timeliness and integrity
* integrity is much more important than timeliness
* correctness of dataflow systems
* Loosely interpreted constraints
* trust but verify
* Maintaining integrity in the face of software bugs
* Don't just blindly trust what they promise
* a culture of verification
* designing for auditability
* the end-to-end argument again
* tools for auditable data systems
* doing the right thing
* predictive analytics
* Privacy and tracking
* surveillance
* consent and freedom of choice
* privacy and use of data
* data as assets and power
* remembering the industrial revolution
* legislation and self-regulation


 

 


